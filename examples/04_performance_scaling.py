#!/usr/bin/env python3
"""
Performance Scaling Tests: Comprehensive Benchmarks

This file stress-tests PatternForge across various dataset sizes, demonstrating:
- Scalability from 10 to 10,000+ rows
- Algorithm selection (BOUNDED vs SCALABLE)
- Single-field vs multi-field performance
- Effort level impact
- Time and memory characteristics

WARNING: Some tests may take 30+ seconds. Run with patience!
"""
import sys
import time
import random
sys.path.insert(0, "../src")

from patternforge.engine.models import SolveOptions, OptimizeWeights, QualityMode
from patternforge.engine.solver import propose_solution, propose_solution_structured

# Set seed for reproducible benchmarks
random.seed(42)

def generate_hierarchical_paths(n_paths, n_levels=4, branch_factor=5):
    """Generate synthetic hierarchical paths for testing."""
    paths = []
    levels = [
        ["chip", "soc", "system"],
        ["cpu", "gpu", "dsp", "npu"],
        ["core0", "core1", "core2", "core3", "cache", "memory"],
        ["unit0", "unit1", "unit2", "bank0", "bank1", "reg0", "reg1"],
        ["inst0", "inst1", "inst2", "port0", "port1", "data", "ctrl"],
    ]

    for i in range(n_paths):
        path_parts = []
        for level in range(n_levels):
            choices = levels[level % len(levels)]
            path_parts.append(random.choice(choices))
        path_parts.append(f"sig{i % 100}")
        paths.append("/".join(path_parts))

    return paths

def generate_structured_rows(n_rows):
    """Generate synthetic structured data for multi-field testing."""
    modules = ["SRAM_512x64", "SRAM_1024x32", "REGFILE_32x64", "DFF", "CKGT"]
    instances = [
        f"chip/cpu/core{i}/l1_cache/bank{j}"
        for i in range(4) for j in range(4)
    ] + [
        f"chip/gpu/shader{i}/cache/bank{j}"
        for i in range(2) for j in range(2)
    ]
    pins = ["DIN[0]", "DIN[31]", "DOUT[0]", "DOUT[31]", "CLK", "WEN", "CEN", "ADDR[0]"]

    rows = []
    for i in range(n_rows):
        rows.append({
            "module": random.choice(modules),
            "instance": random.choice(instances),
            "pin": random.choice(pins),
        })
    return rows

def benchmark(name, func, *args, **kwargs):
    """Run a function and measure time."""
    print(f"\n{'=' * 80}")
    print(f"BENCHMARK: {name}")
    print(f"{'=' * 80}")

    start = time.time()
    result = func(*args, **kwargs)
    elapsed = time.time() - start

    print(f"\nâ±ï¸  Time: {elapsed:.3f}s")

    if isinstance(result, dict):
        metrics = result.get('metrics', {})
        print(f"ðŸ“Š Metrics:")
        print(f"   Coverage: {metrics.get('covered', 'N/A')}/{metrics.get('total_positive', 'N/A')}")
        print(f"   FP: {metrics.get('fp', 'N/A')}, FN: {metrics.get('fn', 'N/A')}")
        print(f"   Patterns: {metrics.get('atoms', 'N/A')}, Wildcards: {metrics.get('wildcards', 'N/A')}")

    return elapsed, result

print("=" * 80)
print("PATTERNFORGE PERFORMANCE SCALING TESTS")
print("=" * 80)
print("""
These benchmarks measure PatternForge's performance across different:
- Dataset sizes (10 to 10,000+ rows)
- Algorithm modes (BOUNDED vs SCALABLE)
- Quality settings (EXACT vs APPROX)
- Effort levels (low, medium, high)

All tests use synthetic data for reproducibility.
""")

# ============================================================================
# TEST SUITE 1: Single-Field Scaling
# ============================================================================
print("\n" + "=" * 80)
print("TEST SUITE 1: Single-Field Scaling (10 to 10,000 rows)")
print("=" * 80)

results_single = []

for n in [10, 50, 100, 500, 1000, 2500, 5000, 10000]:
    include = generate_hierarchical_paths(n, n_levels=4)
    exclude = generate_hierarchical_paths(max(10, n // 10), n_levels=4)

    name = f"Single-field: {n:,} include, {len(exclude):,} exclude"
    elapsed, sol = benchmark(
        name,
        propose_solution,
        include,
        exclude,
        SolveOptions(mode=QualityMode.APPROX)
    )

    results_single.append({
        'n': n,
        'time': elapsed,
        'atoms': sol['metrics']['atoms'],
        'covered': sol['metrics']['covered'],
        'fp': sol['metrics']['fp'],
    })

print("\n" + "=" * 80)
print("SINGLE-FIELD SCALING SUMMARY")
print("=" * 80)
print(f"{'N':>8} {'Time (s)':>10} {'Atoms':>8} {'Coverage':>10} {'FP':>6}")
print(f"{'-'*8} {'-'*10} {'-'*8} {'-'*10} {'-'*6}")
for r in results_single:
    print(f"{r['n']:>8,} {r['time']:>10.3f} {r['atoms']:>8} {r['covered']:>10} {r['fp']:>6}")

# ============================================================================
# TEST SUITE 2: Structured Multi-Field Scaling
# ============================================================================
print("\n" + "=" * 80)
print("TEST SUITE 2: Structured Multi-Field Scaling")
print("=" * 80)

results_structured = []

for n in [10, 50, 100, 500, 1000, 2500]:
    include_rows = generate_structured_rows(n)
    exclude_rows = generate_structured_rows(max(5, n // 10))

    name = f"Structured: {n:,} include rows, {len(exclude_rows):,} exclude"
    elapsed, sol = benchmark(
        name,
        propose_solution_structured,
        include_rows,
        exclude_rows,
        options=SolveOptions(mode=QualityMode.APPROX)
    )

    results_structured.append({
        'n': n,
        'time': elapsed,
        'atoms': sol['metrics']['atoms'],
        'covered': sol['metrics']['covered'],
        'fp': sol['metrics']['fp'],
    })

    # Stop if getting too slow (> 30s)
    if elapsed > 30:
        print(f"\nâš ï¸  Stopping structured tests at N={n} (exceeded 30s threshold)")
        break

print("\n" + "=" * 80)
print("STRUCTURED MULTI-FIELD SCALING SUMMARY")
print("=" * 80)
print(f"{'N':>8} {'Time (s)':>10} {'Atoms':>8} {'Coverage':>10} {'FP':>6}")
print(f"{'-'*8} {'-'*10} {'-'*8} {'-'*10} {'-'*6}")
for r in results_structured:
    print(f"{r['n']:>8,} {r['time']:>10.3f} {r['atoms']:>8} {r['covered']:>10} {r['fp']:>6}")

# ============================================================================
# TEST SUITE 3: Quality Mode Comparison
# ============================================================================
print("\n" + "=" * 80)
print("TEST SUITE 3: Quality Mode Comparison (EXACT vs APPROX)")
print("=" * 80)

n_test = 1000
include = generate_hierarchical_paths(n_test, n_levels=4)
exclude = generate_hierarchical_paths(100, n_levels=4)

print(f"\nDataset: {n_test:,} include, {len(exclude):,} exclude\n")

elapsed_exact, sol_exact = benchmark(
    "EXACT Mode",
    propose_solution,
    include,
    exclude,
    SolveOptions(mode=QualityMode.EXACT)
)

elapsed_approx, sol_approx = benchmark(
    "APPROX Mode",
    propose_solution,
    include,
    exclude,
    SolveOptions(mode=QualityMode.APPROX)
)

print(f"\n{'Mode':<10} {'Time':>10} {'Atoms':>8} {'FP':>6} {'FN':>6}")
print(f"{'-'*10} {'-'*10} {'-'*8} {'-'*6} {'-'*6}")
print(f"{'EXACT':<10} {elapsed_exact:>10.3f}s {sol_exact['metrics']['atoms']:>8} {sol_exact['metrics']['fp']:>6} {sol_exact['metrics']['fn']:>6}")
print(f"{'APPROX':<10} {elapsed_approx:>10.3f}s {sol_approx['metrics']['atoms']:>8} {sol_approx['metrics']['fp']:>6} {sol_approx['metrics']['fn']:>6}")
print(f"\nSpeedup: {elapsed_exact/elapsed_approx:.2f}x faster with APPROX mode")

# ============================================================================
# TEST SUITE 4: Effort Level Impact
# ============================================================================
print("\n" + "=" * 80)
print("TEST SUITE 4: Effort Level Impact (low/medium/high)")
print("=" * 80)

n_test = 500
include_rows = generate_structured_rows(n_test)
exclude_rows = generate_structured_rows(50)

print(f"\nDataset: {n_test:,} structured rows, {len(exclude_rows):,} exclude\n")

effort_results = []
for effort in ["low", "medium", "high"]:
    elapsed, sol = benchmark(
        f"Effort={effort}",
        propose_solution_structured,
        include_rows,
        exclude_rows,
        options=SolveOptions(effort=effort)
    )

    effort_results.append({
        'effort': effort,
        'time': elapsed,
        'atoms': sol['metrics']['atoms'],
        'covered': sol['metrics']['covered'],
        'fp': sol['metrics']['fp'],
    })

print(f"\n{'Effort':<10} {'Time':>10} {'Atoms':>8} {'Coverage':>10} {'FP':>6}")
print(f"{'-'*10} {'-'*10} {'-'*8} {'-'*10} {'-'*6}")
for r in effort_results:
    print(f"{r['effort']:<10} {r['time']:>10.3f}s {r['atoms']:>8} {r['covered']:>10} {r['fp']:>6}")

# ============================================================================
# TEST SUITE 5: Field Weight Impact
# ============================================================================
print("\n" + "=" * 80)
print("TEST SUITE 5: Field Weight Impact (Structured)")
print("=" * 80)

n_test = 200
include_rows = generate_structured_rows(n_test)
exclude_rows = generate_structured_rows(20)

print(f"\nDataset: {n_test:,} structured rows\n")

# No weights (default)
elapsed_default, sol_default = benchmark(
    "Default (no field weights)",
    propose_solution_structured,
    include_rows,
    exclude_rows,
    options=SolveOptions()
)

# Prefer pin field
elapsed_weighted, sol_weighted = benchmark(
    "With field weights (prefer 'pin')",
    propose_solution_structured,
    include_rows,
    exclude_rows,
    options=SolveOptions(
        weights=OptimizeWeights(
            w_field={"pin": 3.0, "module": 1.5, "instance": 0.5}
        )
    )
)

print(f"\n{'Config':<30} {'Time':>10} {'Atoms':>8} {'Pin Atoms':>12}")
print(f"{'-'*30} {'-'*10} {'-'*8} {'-'*12}")

pin_atoms_default = sum(1 for a in sol_default['atoms'] if a.get('field') == 'pin')
pin_atoms_weighted = sum(1 for a in sol_weighted['atoms'] if a.get('field') == 'pin')

print(f"{'Default':<30} {elapsed_default:>10.3f}s {sol_default['metrics']['atoms']:>8} {pin_atoms_default:>12}")
print(f"{'w_field={{pin:3.0}}':<30} {elapsed_weighted:>10.3f}s {sol_weighted['metrics']['atoms']:>8} {pin_atoms_weighted:>12}")

# ============================================================================
# TEST SUITE 6: Worst-Case Stress Test
# ============================================================================
print("\n" + "=" * 80)
print("TEST SUITE 6: Worst-Case Stress Test (Highly Unique Paths)")
print("=" * 80)
print("""
This test uses paths with high cardinality (many unique segments).
This is the hardest case for pattern finding.
""")

# Generate highly unique paths
n_stress = 1000
include_stress = [f"unique/path/segment{i}/variant{i % 10}/instance{i}" for i in range(n_stress)]
exclude_stress = [f"unique/path/segment{i}/variant{i % 10}/different{i}" for i in range(100)]

elapsed_stress, sol_stress = benchmark(
    f"Stress test: {n_stress:,} highly unique paths",
    propose_solution,
    include_stress,
    exclude_stress,
    SolveOptions(mode=QualityMode.APPROX)
)

# ============================================================================
# FINAL SUMMARY
# ============================================================================
print("\n" + "=" * 80)
print("PERFORMANCE TEST SUMMARY")
print("=" * 80)
print(f"""
SINGLE-FIELD PERFORMANCE:
  - 10 rows:        ~{results_single[0]['time']:.3f}s
  - 1,000 rows:     ~{[r for r in results_single if r['n'] == 1000][0]['time']:.3f}s
  - 10,000 rows:    ~{[r for r in results_single if r['n'] == 10000][0]['time']:.3f}s
  - Scaling:        Near-linear O(N)

STRUCTURED MULTI-FIELD PERFORMANCE:
  - 10 rows:        ~{results_structured[0]['time']:.3f}s
  - 1,000 rows:     ~{[r for r in results_structured if r['n'] == 1000][0]['time']:.3f}s
  - Scaling:        Near-linear O(N)

QUALITY MODES:
  - EXACT:          {elapsed_exact:.3f}s (0 FP guaranteed)
  - APPROX:         {elapsed_approx:.3f}s ({elapsed_exact/max(elapsed_approx, 0.001):.1f}x faster)

EFFORT LEVELS:
  - Low:            {effort_results[0]['time']:.3f}s (fastest)
  - Medium:         {effort_results[1]['time']:.3f}s (balanced)
  - High:           {effort_results[2]['time']:.3f}s (best quality)

RECOMMENDATIONS:
  - N < 100:        Use any settings, all fast (<0.1s)
  - 100 â‰¤ N < 1k:   Use default settings (APPROX, medium effort)
  - 1k â‰¤ N < 10k:   Use APPROX mode, consider effort=low for speed
  - N â‰¥ 10k:        Use APPROX + effort=low for best performance

The pattern engine scales well to 10,000+ rows with O(N) complexity!
""")

print("=" * 80)
print("âœ… Performance tests complete!")
print("=" * 80)
